from time import sleep
from typing import List, Optional

from tokenizers import Tokenizer

tokenizer = Tokenizer.from_pretrained("Cohere/command-nightly")


class DummyModel:
    """DummyModel class for generating tokens."""

    # default value if no text provided; these tokens are the
    # encoding for the text: "Hello, world! ðŸ˜„"
    tokens = [28339, 19, 3845, 8, 80503, 234]
    # default number of tokens that will be generated
    num_tokens = 30

    def token_generator(self):
        """Token generator method.

        Yields:
            int: token
        """
        num_tokens = DummyModel.num_tokens
        for i in range(num_tokens):
            sleep(0.05)  # 50ms
            yield (DummyModel.tokens[i % len(DummyModel.tokens)])


def encode_text(text: str, num_tokens: Optional[int]):
    """Sets the text for our fake generator (DummyModel), so
    that the next call to decode will decode the provided text.
    Also optionally sets the number of tokens that will be generated.

    Args:
        text (str): text for which the _subsequent_ call to decode() will generate tokens.
        num_tokens (Optional[int]): number of tokens that will be generated by the next
        call to decode().

    Raises:
        ValueError: exception raised in case the tokenizer fails to encode the text,
        for any reason.
    """
    try:
        encoded_text = tokenizer.encode(text)
        # skip the BOS_token
        DummyModel.tokens = encoded_text.ids[1:]
    except Exception as exc:
        # in real world code, should also log the exception
        raise ValueError(
            f"Error: failed to encode provided text {text}, exception: {str(exc)}."
        )
    DummyModel.num_tokens = num_tokens
    return


def decode_tokens(tokens: List[int]):
    """Decodes the provided tokens.

    Args:
        tokens (List[int]): tokens to be decoded.

    Returns:
        _type_: decoded text from the provided tokens.
    """
    return tokenizer.decode(tokens)


def generate_tokens(stop_sequences: List[str] = None, end_sequences: List[str] = None):
    """Invoke the token generator and decode the tokens, supporting stop sequences,
    end sequences, and unicode characters as described in the problem statement.

    Args:
        stop_sequences (List[str], optional): list of words that will stop the token generator.
        The word itself will be returned.
        end_sequences (List[str], optional): list of words that will stop the token generator.
        The word itself will not be returned.

    Yields:
        _type_: decoded word from one or two (for unicode support) tokens
    """
    generator = DummyModel().token_generator()
    try:
        while generator:
            token_id = next(generator)
            decoded_token = decode_tokens([token_id])
            return_candidate = None
            if decoded_token.strip().isascii():
                return_candidate = decoded_token
            else:
                # if the decoded_token is the first of a pair of tokens
                # representing a unicode character
                if generator:
                    second_token_id = next(generator)
                    combined_token = decode_tokens([token_id, second_token_id])
                    return_candidate = combined_token
                else:
                    # non-ascii char is cut in the middle, return the first part anyway
                    return_candidate = decoded_token
            if end_sequences and return_candidate.strip() in end_sequences:
                return
            if stop_sequences and return_candidate.strip() in stop_sequences:
                yield return_candidate
                return
            yield return_candidate
    except StopIteration:
        return
