from typing import List

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from tokenizers import Tokenizer

from .model_utils.model_utils import encode_text, generate_tokens

tokenizer = Tokenizer.from_pretrained("Cohere/command-nightly")
app = FastAPI()


@app.get("/health")
async def health() -> dict:
    return {"status": "ok"}


@app.post("/encode")
async def encode(text: str, num_tokens: int = 30):
    """Endpoint to encode given text. The _next_ call to the
    /decode endpoint will generate tokens based on this provided text.

    Args:
        text (str): text from which the next call to decode will generated tokens.
        num_tokens (int, optional): number of tokens to be generated by the
        next call to decode. Defaults to 30.

    Raises:
        HTTPException: raises exception in case the model fails to encode the text.

    Returns:
        _type_: typical HTTP response enclosing a json object.
    """
    try:
        encode_text(text, num_tokens)
    except ValueError as exc:
        # you can do fancier stuff here:
        # type, value, traceback = sys.exc_info()
        raise HTTPException(status_code=400, detail=str(exc))
    return {"Successfully encoded the provided text."}


@app.post("/decode")
async def decode(stop_sequences: List[str] = None, end_sequences: List[str] = None):
    """Endpoint to decode text that was provided by a previous call to the
    /encode endpoint, or the default text if there was no previous call to the
    /encode endpoint.

    Args:
        stop_sequences (List[str], optional): The generated text will be cut at
        the end of the earliest occurrence of a stop sequence. The sequence
        will be included the generated text Defaults to None.
        end_sequences (List[str], optional): works like stop_sequences, but
        excludes the string from the returned result. Defaults to None.

    Returns:
        _type_: a StreamingResponse, streams one decoded word at a time.
    """
    return StreamingResponse(
        generate_tokens(stop_sequences, end_sequences), media_type="text/plain"
    )
